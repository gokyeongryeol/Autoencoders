{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid, save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_threads = 8\n",
    "\n",
    "inp_dim = 784\n",
    "hid_dim = 1000\n",
    "out_dim = 10\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader():\n",
    "    train_data = torchvision.datasets.MNIST(root='./data', \n",
    "                                            train=True, \n",
    "                                            transform=transforms.ToTensor(), \n",
    "                                            download=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               num_workers=num_threads)\n",
    "    test_data = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=False, \n",
    "                                           transform=transforms.ToTensor(), \n",
    "                                           download=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                              batch_size=batch_size, \n",
    "                                              shuffle=False, \n",
    "                                              num_workers=num_threads)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_plot(outputs, nrow=10):\n",
    "    outputs = outputs.view(-1, 1, 28, 28)\n",
    "    save_image(outputs, filename='plots/outputs_%d.png' % (int(k)), nrow=nrow, padding=1, scale_each=True)\n",
    "\n",
    "def filter_plot(model, nrow=30):\n",
    "    weights = model.W.detach().clone()\n",
    "    weights = torch.t(weights)\n",
    "    weights = weights.view(hid_dim, 1, 28, 28)\n",
    "    weights = weights[0:120]\n",
    "    save_image(weights, filename='plots/filters_%d.png' % (k),\n",
    "               nrow=nrow, padding=1, normalize=True, scale_each=True)\n",
    "    \n",
    "def plot_activations(hid):\n",
    "    activations = hid.to(torch.device(\"cpu\")).numpy()\n",
    "    plt.hist(activations, histtype='barstacked')\n",
    "    plt.yscale('log')\n",
    "    plt.yticks([10**1, 10**2, 10**3, 10**4, 10**5, 10**6, 10**7],\n",
    "                [1, 2, 3, 4, 5, 6, 7])\n",
    "    plt.xlim(0, 3)\n",
    "    plt.grid(True)\n",
    "    plt.savefig('plots/activations_%d.png' % (k))\n",
    "    plt.close()\n",
    "    \n",
    "def plot_loss_curve(k, train_list, test_list, ylim):   \n",
    "    plt.plot(train_list, 'r-', label='train loss')\n",
    "    plt.plot(test_list, 'r--', label='test loss')\n",
    "    plt.legend()\n",
    "    plt.ylim(ylim)\n",
    "    plt.savefig('plots/loss_curves_%d.png' % (k))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KsparseAE(nn.Module):\n",
    "    def __init__(self, supervised=False, inp_dim=inp_dim, hid_dim=hid_dim, out_dim=out_dim):\n",
    "        super(KsparseAE, self).__init__()\n",
    "        self.supervised = supervised\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.b = torch.nn.Parameter(torch.empty((self.hid_dim), requires_grad=True))\n",
    "        self.c = torch.nn.Parameter(torch.empty((self.inp_dim), requires_grad=True))\n",
    "        self.W = torch.nn.Parameter(torch.empty((self.inp_dim, self.hid_dim), requires_grad=True))\n",
    "        torch.nn.init.normal_(self.b, std=0.01)\n",
    "        torch.nn.init.normal_(self.c, std=0.01)\n",
    "        torch.nn.init.normal_(self.W, std=0.01)\n",
    "        self.layer = nn.Linear(self.hid_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hid = self.b + torch.matmul(x, self.W)\n",
    "        values, indices = torch.kthvalue(-hid, k, keepdim=True)\n",
    "        hid[hid < -values] = 0\n",
    "        self.hid = hid\n",
    "        if self.supervised:\n",
    "            y = self.layer(self.hid)\n",
    "        elif not self.supervised:    \n",
    "            y = self.c + torch.matmul(self.hid, torch.t(self.W))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, supervised=False):\n",
    "    train_loader, test_loader = data_loader()\n",
    "        \n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    train_list, test_list = [], []\n",
    "    hid = torch.tensor([0.0]).to(device)\n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        cnt = 0\n",
    "        for x, y in train_loader:\n",
    "            cnt += 1\n",
    "            inputs, labels = x, y\n",
    "            new_batch_size = inputs.size()[0]\n",
    "            inputs = inputs.view(new_batch_size, -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            hid = torch.cat((hid, model.hid.detach()[:,0]))\n",
    "            if supervised:\n",
    "                loss = criterion(outputs, labels)\n",
    "            elif not supervised:\n",
    "                loss = criterion(outputs, inputs)\n",
    "            train_loss += loss * new_batch_size / batch_size\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss = train_loss / cnt\n",
    "        train_list.append(train_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            cnt = 0\n",
    "            length = 0\n",
    "            accuracy = 0\n",
    "            for x, y in test_loader:\n",
    "                cnt += 1\n",
    "                length += len(x)\n",
    "                inputs, labels = x, y\n",
    "                new_batch_size = x.size()[0]\n",
    "                inputs = inputs.view(new_batch_size, -1).to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                if supervised:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    accuracy += sum(torch.argmax(outputs, dim=1) == labels)\n",
    "                elif not supervised:\n",
    "                    loss = criterion(outputs, inputs)\n",
    "                test_loss += loss * new_batch_size / batch_size\n",
    "            test_loss = test_loss / cnt\n",
    "            test_list.append(test_loss)\n",
    "            \n",
    "        torch.save(model.state_dict(), 'models/KsparseAE_%d.pt' % (k))\n",
    "            \n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print('[Epoch %d] train_loss: %.3f, test_loss: %.3f' % (epoch+1, train_loss, test_loss))\n",
    "    \n",
    "    filter_plot(model)  #1st experiment in the report : drawing a filter plot for the first weight matrix.\n",
    "    \n",
    "    if not supervised:\n",
    "        #output_plot(outputs)\n",
    "        hid = hid[1:]\n",
    "        print(hid.size())\n",
    "        plot_activations(hid) #2nd experiment in the report : drawing the log-histogram of the hidden unit activities\n",
    "    \n",
    "    if supervised:\n",
    "        accuracy = accuracy.item() / length * 100\n",
    "        print('accuracy is', accuracy,'%')\n",
    "\n",
    "        with open('loss/train_ELBO_%d.txt' % (k), 'wb') as f:\n",
    "            pickle.dump(train_list, f)\n",
    "        with open('loss/test_ELBO_%d.txt' % (k), 'wb') as f:\n",
    "            pickle.dump(test_list, f)\n",
    "\n",
    "    #plot_loss_curve(k, train_list, test_list, (0, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "[Epoch 1] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 2] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 3] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 4] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 5] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 6] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 7] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 8] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 9] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 10] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 11] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 12] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 13] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 14] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 15] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 16] train_loss: 0.002, test_loss: 0.000\n",
      "[Epoch 17] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 18] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 19] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 20] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 21] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 22] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 23] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 24] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 25] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 26] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 27] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 28] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 29] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 30] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 31] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 32] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 33] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 34] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 35] train_loss: 0.001, test_loss: 0.000\n",
      "[Epoch 36] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 37] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 38] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 39] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 40] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 41] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 42] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 43] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 44] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 45] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 46] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 47] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 48] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 49] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 50] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 51] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 52] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 53] train_loss: 0.001, test_loss: 0.000\n",
      "[Epoch 54] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 55] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 56] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 57] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 58] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 59] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 60] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 61] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 62] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 63] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 64] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 65] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 66] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 67] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 68] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 69] train_loss: 0.003, test_loss: 0.000\n",
      "[Epoch 70] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 71] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 72] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 73] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 74] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 75] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 76] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 77] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 78] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 79] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 80] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 81] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 82] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 83] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 84] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 85] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 86] train_loss: 0.001, test_loss: 0.000\n",
      "[Epoch 87] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 88] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 89] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 90] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 91] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 92] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 93] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 94] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 95] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 96] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 97] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 98] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 99] train_loss: 0.000, test_loss: 0.000\n",
      "[Epoch 100] train_loss: 0.000, test_loss: 0.000\n",
      "torch.Size([6000000])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(device)\n",
    "\n",
    "    supervised=False\n",
    "    \n",
    "    k = 70\n",
    "    lr=1e-3\n",
    "    n_epoch = 100\n",
    "    \n",
    "    model = KsparseAE(supervised=supervised).to(device)\n",
    "    model.load_state_dict(torch.load('models/KsparseAE_%d.pt' % (k)))\n",
    "    train(model=model, supervised=supervised)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
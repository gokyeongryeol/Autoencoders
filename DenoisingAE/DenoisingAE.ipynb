{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_threads = 8\n",
    "\n",
    "inp_dim = 784\n",
    "hid_dim = 120\n",
    "out_dim = 10\n",
    "\n",
    "hid_dim1 = 256\n",
    "hid_dim2 = 128\n",
    "hid_dim3 = 32\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(txt_file):\n",
    "    if txt_file==None:\n",
    "        train_data = torchvision.datasets.MNIST(root='./data', \n",
    "                                                train=True, \n",
    "                                                transform=transforms.ToTensor(),\n",
    "                                                download=True)\n",
    "        test_data = torchvision.datasets.MNIST(root='./data', \n",
    "                                               train=False, \n",
    "                                               transform=transforms.ToTensor(), \n",
    "                                               download=True)\n",
    "    else:\n",
    "        train_data = MNIST_variation(txt_file='./data/'+txt_file+'_train.amat')\n",
    "        test_data = MNIST_variation(txt_file='./data/'+txt_file+'_test.amat')\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               num_workers=num_threads)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                              batch_size=batch_size, \n",
    "                                              shuffle=False, \n",
    "                                              num_workers=num_threads)   \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_variation(Dataset):\n",
    "    def __init__(self, txt_file):\n",
    "        raw_data = np.loadtxt(txt_file)\n",
    "        self.dataset = raw_data\n",
    "        x = torch.from_numpy(np.array(self.dataset[:, :-1], dtype=np.float32))\n",
    "        self.x = x.view(-1,28,28)\n",
    "        self.y = torch.from_numpy(np.array(self.dataset[:, -1], dtype=np.int_))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_plot(outputs, nrow=10):\n",
    "    outputs = outputs.view(-1, 1, 28, 28)\n",
    "    save_image(outputs, filename='plots/outputs_%d.png' % (int(portion*100)), \n",
    "               nrow=nrow, padding=1, scale_each=True)\n",
    "\n",
    "def filter_plot(model, nrow=12):\n",
    "    weights = model.W.detach().clone()\n",
    "    filters = torch.t(weights)\n",
    "    filters = filters.view(hid_dim, 1, 28, 28)\n",
    "    save_image(filters, filename='plots/filters_%d.png' % (int(portion*100)),\n",
    "               nrow=nrow, padding=1, normalize=True, scale_each=False)\n",
    "\n",
    "def plot_loss_curve(portion, train_list, test_list):\n",
    "    plt.plot(train_list, 'r-', label='train loss')\n",
    "    plt.plot(test_list, 'r--', label='test loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/loss_curves_%d.png' % (int(portion*100)))\n",
    "    plt.close()\n",
    "\n",
    "def plot_accur_curve(portion, accur_list):\n",
    "    plt.plot(accur_list, 'b-', label='accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/accuracy_%d.png' % (int(portion*100)))\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAE(nn.Module):\n",
    "    def __init__(self, mode='train', supervised=False, inp_dim=inp_dim, hid_dim=hid_dim, out_dim=out_dim):\n",
    "        super(DenoisingAE, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.supervised = supervised\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.b = torch.nn.Parameter(torch.zeros([self.hid_dim], requires_grad=True))\n",
    "        self.c = torch.nn.Parameter(torch.zeros([self.inp_dim], requires_grad=True))\n",
    "        self.W = torch.nn.Parameter(torch.empty((self.inp_dim, self.hid_dim), requires_grad=True))\n",
    "        torch.nn.init.uniform_(self.W, a=-1/self.inp_dim, b=1/self.inp_dim)\n",
    "        self.layer = nn.Linear(self.hid_dim, self.out_dim)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encoderNet(self, x):\n",
    "        hid = self.sigmoid(self.b + torch.matmul(x, self.W))\n",
    "        return hid\n",
    "        \n",
    "    def decoderNet(self, hid):\n",
    "        out = self.sigmoid(self.c + torch.matmul(hid, torch.t(self.W)))\n",
    "        return out\n",
    "        \n",
    "    def corruption(self, inputs):\n",
    "        new_batch_size = inputs.size()[0]\n",
    "        base = np.array([])\n",
    "        for i in range(new_batch_size):\n",
    "            tmp = np.random.choice([0.0, 1.0], size=self.inp_dim, p=[portion, 1-portion])\n",
    "            base = np.append(base, tmp)\n",
    "        base = np.reshape(base, [new_batch_size, self.inp_dim])\n",
    "        base = torch.from_numpy(base).float().to(device)\n",
    "        corr_inputs = inputs * base\n",
    "        return corr_inputs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.mode == 'train':\n",
    "            corr_inputs = self.corruption(x)\n",
    "            hid = self.encoderNet(corr_inputs)\n",
    "        elif self.mode == 'test':\n",
    "            hid = self.encoderNet(x)\n",
    "        if self.supervised:\n",
    "            y = self.layer(hid)\n",
    "        elif not self.supervised:\n",
    "            y = self.decoderNet(hid)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAE_train(model, supervised=False, txt_file=None):\n",
    "    train_loader, test_loader = data_loader(txt_file)\n",
    "    \n",
    "    if supervised:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif not supervised:\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    train_list, test_list = [], []\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        model.mode = 'train'\n",
    "        train_loss = 0.0\n",
    "        cnt = 0\n",
    "        for x, y in train_loader:\n",
    "            cnt += 1\n",
    "            inputs, labels = x, y\n",
    "            new_batch_size = inputs.size()[0]\n",
    "            inputs = inputs.view(new_batch_size, -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            if supervised:\n",
    "                loss = criterion(outputs, labels)\n",
    "            elif not supervised:\n",
    "                loss = criterion(outputs, inputs)\n",
    "            train_loss += loss * new_batch_size / batch_size\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss = train_loss / cnt\n",
    "        train_list.append(train_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            model.mode = 'test'\n",
    "            test_loss = 0.0\n",
    "            cnt = 0\n",
    "            for x, y in test_loader:\n",
    "                cnt += 1\n",
    "                inputs, labels = x, y\n",
    "                new_batch_size = x.size()[0]\n",
    "                inputs = inputs.view(new_batch_size, -1).to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                if supervised:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                elif not supervised:\n",
    "                    loss = criterion(outputs, inputs)\n",
    "                test_loss += loss * new_batch_size / batch_size\n",
    "            \n",
    "            test_loss = test_loss / cnt\n",
    "            test_list.append(test_loss)\n",
    "            \n",
    "        torch.save(model.state_dict(), 'models/DenoisingAE_%d.pt' % (int(portion*100)))\n",
    "        model.mode = 'train'\n",
    "        \n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print('[Epoch %d] train_loss: %.3f, test_loss: %.3f' % (epoch+1, train_loss, test_loss))\n",
    "    \n",
    "    if not supervised:\n",
    "        output_plot(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1st experiment : filter plot in unsupervised learning setting.\n",
    "if __name__ == '__main__':\n",
    "    print(device)\n",
    "    \n",
    "    portion = 0.4\n",
    "    lr = 1e-3\n",
    "    n_epoch = 5\n",
    "    \n",
    "    model = DenoisingAE().to(device)\n",
    "    model.load_state_dict(torch.load('models/DenoisingAE_%d.pt' % (int(portion*100))))\n",
    "    DAE_train(model=model)\n",
    "    filter_plot(model)\n",
    "   "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SdA_3(nn.Module):\n",
    "    def __init__(self, step):\n",
    "        super(SdA_3, self).__init__()\n",
    "        self.step = step\n",
    "        if self.step == 1:\n",
    "            bool1, bool2, bool3 = False, False, False\n",
    "        elif self.step == 2:\n",
    "            bool1, bool2, bool3 = True, False, False\n",
    "        elif self.step == 3:\n",
    "            bool1, bool2, bool3 = True, True, False\n",
    "        elif self.step == 4:\n",
    "            bool1, bool2, bool3 = True, True, True\n",
    "        \n",
    "        self.layer1 = DenoisingAE(supervised=bool1, inp_dim=inp_dim, hid_dim=hid_dim1, out_dim=hid_dim2)\n",
    "        self.layer2 = DenoisingAE(supervised=bool2, inp_dim=hid_dim1, hid_dim=hid_dim2, out_dim=hid_dim3)\n",
    "        self.layer3 = DenoisingAE(supervised=bool3, inp_dim=hid_dim2, hid_dim=hid_dim3, out_dim=out_dim)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.step == 1:\n",
    "            hid1 = self.layer1(x)\n",
    "            y = hid1\n",
    "        elif self.step == 2:\n",
    "            hid1 = self.layer1.encoderNet(x)\n",
    "            hid2 = self.layer2(hid1)\n",
    "            y = self.layer1.decoderNet(hid2)\n",
    "        elif self.step == 3:\n",
    "            hid1 = self.layer1.encoderNet(x)\n",
    "            hid2 = self.layer2.encoderNet(hid1)\n",
    "            hid3 = self.layer3(hid2)\n",
    "            y_ = self.layer2.decoderNet(hid3)\n",
    "            y = self.layer1.decoderNet(y_)\n",
    "        elif self.step == 4:\n",
    "            hid1 = self.layer1.encoderNet(x)\n",
    "            hid2 = self.layer2.encoderNet(hid1)\n",
    "            out = self.layer3(hid2)\n",
    "            y = out\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SdA_train(model, supervised=False, txt_file=None):\n",
    "    train_loader, test_loader = data_loader(txt_file)\n",
    "    \n",
    "    if supervised:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif not supervised:\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    train_list, test_list, accur_list = [], [], []\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        model.mode = 'train'\n",
    "        train_loss = 0.0\n",
    "        cnt = 0\n",
    "        for x, y in train_loader:\n",
    "            cnt += 1\n",
    "            inputs, labels = x, y\n",
    "            new_batch_size = inputs.size()[0]\n",
    "            inputs = inputs.view(new_batch_size, -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            if supervised:\n",
    "                loss = criterion(outputs, labels)\n",
    "            elif not supervised:\n",
    "                loss = criterion(outputs, inputs)\n",
    "            train_loss += loss * new_batch_size / batch_size\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss = train_loss / cnt\n",
    "        train_list.append(train_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            model.mode = 'test'\n",
    "            test_loss = 0.0\n",
    "            cnt = 0\n",
    "            length = 0\n",
    "            accur = torch.tensor(0.0)\n",
    "            for x, y in test_loader:\n",
    "                cnt += 1\n",
    "                length += len(x)\n",
    "                inputs, labels = x, y\n",
    "                new_batch_size = x.size()[0]\n",
    "                inputs = inputs.view(new_batch_size, -1).to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                if supervised:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    accur += sum(torch.argmax(outputs, dim=1) == labels)\n",
    "                elif not supervised:\n",
    "                    loss = criterion(outputs, inputs)\n",
    "                test_loss += loss * new_batch_size / batch_size\n",
    "            \n",
    "            model.mode = 'train'\n",
    "            accuracy = accur.item() / length * 100\n",
    "            test_loss = test_loss / cnt\n",
    "            test_list.append(test_loss)\n",
    "            accur_list.append(accuracy)\n",
    "            \n",
    "            \n",
    "        torch.save(model.state_dict(), 'models/SdA_3_%d.pt' % (int(portion*100)))\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print('[Epoch %d] train_loss: %.3f, test_loss: %.3f' % (epoch+1, train_loss, test_loss))\n",
    "    \n",
    "    if supervised:\n",
    "        print('accuracy is', max(accur_list),'%')\n",
    "        plot_loss_curve(portion, train_list, test_list)\n",
    "        plot_accur_curve(portion, accur_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_fix(layer):\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def pretrain(txt_file=None):\n",
    "    for i in range(3):\n",
    "        step = i+1\n",
    "        model = SdA_3(step=step).to(device)\n",
    "        if step == 1:\n",
    "            param_fix(model.layer2)\n",
    "            param_fix(model.layer3)\n",
    "        elif step == 2:\n",
    "            param_fix(model.layer1)\n",
    "            param_fix(model.layer3)\n",
    "        elif step == 3:\n",
    "            param_fix(model.layer1)\n",
    "            param_fix(model.layer2)\n",
    "        SdA_train(model=model, supervised=False, txt_file=txt_file)\n",
    "    print('pretraining ends')\n",
    "                \n",
    "def fine_tune(txt_file=None):\n",
    "    model = SdA_3(step=4).to(device)\n",
    "    model.load_state_dict(torch.load('models/SdA_3_%d.pt' % (int(portion*100))))\n",
    "    SdA_train(model=model, supervised=True, txt_file=txt_file)\n",
    "    print('fine tuning ends')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "[Epoch 1] train_loss: 0.653, test_loss: 0.601\n",
      "[Epoch 2] train_loss: 0.587, test_loss: 0.581\n",
      "[Epoch 3] train_loss: 0.580, test_loss: 0.579\n",
      "[Epoch 4] train_loss: 0.579, test_loss: 0.579\n",
      "[Epoch 5] train_loss: 0.578, test_loss: 0.578\n",
      "[Epoch 6] train_loss: 0.578, test_loss: 0.578\n",
      "[Epoch 7] train_loss: 0.578, test_loss: 0.578\n",
      "[Epoch 8] train_loss: 0.578, test_loss: 0.578\n",
      "[Epoch 9] train_loss: 0.577, test_loss: 0.578\n",
      "[Epoch 10] train_loss: 0.577, test_loss: 0.578\n",
      "[Epoch 1] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 2] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 3] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 4] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 5] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 6] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 7] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 8] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 9] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 10] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 1] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 2] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 3] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 4] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 5] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 6] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 7] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 8] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 9] train_loss: 0.693, test_loss: 0.693\n",
      "[Epoch 10] train_loss: 0.693, test_loss: 0.693\n",
      "pretraining ends\n",
      "[Epoch 1] train_loss: 2.303, test_loss: 2.301\n",
      "[Epoch 2] train_loss: 2.301, test_loss: 2.301\n",
      "[Epoch 3] train_loss: 2.301, test_loss: 2.301\n",
      "[Epoch 4] train_loss: 2.301, test_loss: 2.301\n",
      "[Epoch 5] train_loss: 2.301, test_loss: 2.301\n",
      "[Epoch 6] train_loss: 2.301, test_loss: 2.301\n",
      "[Epoch 7] train_loss: 2.246, test_loss: 2.152\n",
      "[Epoch 8] train_loss: 2.115, test_loss: 2.097\n",
      "[Epoch 9] train_loss: 2.067, test_loss: 2.040\n",
      "[Epoch 10] train_loss: 1.920, test_loss: 1.854\n",
      "[Epoch 11] train_loss: 1.831, test_loss: 1.845\n",
      "[Epoch 12] train_loss: 1.803, test_loss: 1.811\n",
      "[Epoch 13] train_loss: 1.790, test_loss: 1.791\n",
      "[Epoch 14] train_loss: 1.780, test_loss: 1.773\n",
      "[Epoch 15] train_loss: 1.767, test_loss: 1.768\n",
      "[Epoch 16] train_loss: 1.751, test_loss: 1.772\n",
      "[Epoch 17] train_loss: 1.738, test_loss: 1.742\n",
      "[Epoch 18] train_loss: 1.719, test_loss: 1.739\n",
      "[Epoch 19] train_loss: 1.707, test_loss: 1.737\n",
      "[Epoch 20] train_loss: 1.691, test_loss: 1.711\n",
      "[Epoch 21] train_loss: 1.676, test_loss: 1.712\n",
      "[Epoch 22] train_loss: 1.661, test_loss: 1.707\n",
      "[Epoch 23] train_loss: 1.649, test_loss: 1.685\n",
      "[Epoch 24] train_loss: 1.634, test_loss: 1.671\n",
      "[Epoch 25] train_loss: 1.621, test_loss: 1.671\n",
      "[Epoch 26] train_loss: 1.610, test_loss: 1.672\n",
      "[Epoch 27] train_loss: 1.596, test_loss: 1.648\n",
      "[Epoch 28] train_loss: 1.585, test_loss: 1.633\n",
      "[Epoch 29] train_loss: 1.573, test_loss: 1.623\n",
      "[Epoch 30] train_loss: 1.559, test_loss: 1.620\n",
      "[Epoch 31] train_loss: 1.548, test_loss: 1.605\n",
      "[Epoch 32] train_loss: 1.534, test_loss: 1.621\n",
      "[Epoch 33] train_loss: 1.523, test_loss: 1.646\n",
      "[Epoch 34] train_loss: 1.511, test_loss: 1.583\n",
      "[Epoch 35] train_loss: 1.493, test_loss: 1.590\n",
      "[Epoch 36] train_loss: 1.479, test_loss: 1.556\n",
      "[Epoch 37] train_loss: 1.462, test_loss: 1.558\n",
      "[Epoch 38] train_loss: 1.445, test_loss: 1.545\n",
      "[Epoch 39] train_loss: 1.427, test_loss: 1.571\n",
      "[Epoch 40] train_loss: 1.411, test_loss: 1.527\n",
      "[Epoch 41] train_loss: 1.395, test_loss: 1.511\n",
      "[Epoch 42] train_loss: 1.378, test_loss: 1.526\n",
      "[Epoch 43] train_loss: 1.360, test_loss: 1.500\n",
      "[Epoch 44] train_loss: 1.345, test_loss: 1.480\n",
      "[Epoch 45] train_loss: 1.332, test_loss: 1.478\n",
      "[Epoch 46] train_loss: 1.311, test_loss: 1.492\n",
      "[Epoch 47] train_loss: 1.306, test_loss: 1.490\n",
      "[Epoch 48] train_loss: 1.288, test_loss: 1.480\n",
      "[Epoch 49] train_loss: 1.270, test_loss: 1.467\n",
      "[Epoch 50] train_loss: 1.262, test_loss: 1.446\n",
      "[Epoch 51] train_loss: 1.244, test_loss: 1.475\n",
      "[Epoch 52] train_loss: 1.230, test_loss: 1.461\n",
      "[Epoch 53] train_loss: 1.216, test_loss: 1.444\n",
      "[Epoch 54] train_loss: 1.202, test_loss: 1.436\n",
      "[Epoch 55] train_loss: 1.193, test_loss: 1.446\n",
      "[Epoch 56] train_loss: 1.180, test_loss: 1.434\n",
      "[Epoch 57] train_loss: 1.160, test_loss: 1.417\n",
      "[Epoch 58] train_loss: 1.152, test_loss: 1.414\n",
      "[Epoch 59] train_loss: 1.138, test_loss: 1.417\n",
      "[Epoch 60] train_loss: 1.130, test_loss: 1.420\n",
      "[Epoch 61] train_loss: 1.110, test_loss: 1.419\n",
      "[Epoch 62] train_loss: 1.103, test_loss: 1.427\n",
      "[Epoch 63] train_loss: 1.092, test_loss: 1.415\n",
      "[Epoch 64] train_loss: 1.075, test_loss: 1.432\n",
      "[Epoch 65] train_loss: 1.067, test_loss: 1.420\n",
      "[Epoch 66] train_loss: 1.059, test_loss: 1.472\n",
      "[Epoch 67] train_loss: 1.046, test_loss: 1.467\n",
      "[Epoch 68] train_loss: 1.039, test_loss: 1.444\n",
      "[Epoch 69] train_loss: 1.024, test_loss: 1.450\n",
      "[Epoch 70] train_loss: 1.013, test_loss: 1.441\n",
      "[Epoch 71] train_loss: 1.006, test_loss: 1.426\n",
      "[Epoch 72] train_loss: 0.996, test_loss: 1.471\n",
      "[Epoch 73] train_loss: 0.978, test_loss: 1.424\n",
      "[Epoch 74] train_loss: 0.975, test_loss: 1.449\n",
      "[Epoch 75] train_loss: 0.969, test_loss: 1.442\n",
      "[Epoch 76] train_loss: 0.954, test_loss: 1.442\n",
      "[Epoch 77] train_loss: 0.946, test_loss: 1.484\n",
      "[Epoch 78] train_loss: 0.933, test_loss: 1.444\n",
      "[Epoch 79] train_loss: 0.929, test_loss: 1.480\n",
      "[Epoch 80] train_loss: 0.919, test_loss: 1.454\n",
      "[Epoch 81] train_loss: 0.905, test_loss: 1.470\n",
      "[Epoch 82] train_loss: 0.904, test_loss: 1.489\n",
      "[Epoch 83] train_loss: 0.894, test_loss: 1.470\n",
      "[Epoch 84] train_loss: 0.880, test_loss: 1.514\n",
      "[Epoch 85] train_loss: 0.880, test_loss: 1.477\n",
      "[Epoch 86] train_loss: 0.863, test_loss: 1.485\n",
      "[Epoch 87] train_loss: 0.859, test_loss: 1.546\n",
      "[Epoch 88] train_loss: 0.847, test_loss: 1.499\n",
      "[Epoch 89] train_loss: 0.845, test_loss: 1.541\n",
      "[Epoch 90] train_loss: 0.833, test_loss: 1.506\n",
      "[Epoch 91] train_loss: 0.826, test_loss: 1.490\n",
      "[Epoch 92] train_loss: 0.821, test_loss: 1.533\n",
      "[Epoch 93] train_loss: 0.812, test_loss: 1.533\n",
      "[Epoch 94] train_loss: 0.807, test_loss: 1.506\n",
      "[Epoch 95] train_loss: 0.800, test_loss: 1.518\n",
      "[Epoch 96] train_loss: 0.790, test_loss: 1.573\n",
      "[Epoch 97] train_loss: 0.785, test_loss: 1.550\n",
      "[Epoch 98] train_loss: 0.778, test_loss: 1.556\n",
      "[Epoch 99] train_loss: 0.768, test_loss: 1.535\n",
      "[Epoch 100] train_loss: 0.760, test_loss: 1.539\n",
      "accuracy is 53.425 %\n",
      "fine tuning ends\n"
     ]
    }
   ],
   "source": [
    "# 2nd experiment : find error rate when applying SdA-3 on MNIST variations\n",
    "if __name__ == '__main__':\n",
    "    print(device)\n",
    "\n",
    "    txt_file = 'rot-bg-img'\n",
    "    n_epoch = 10\n",
    "    lr = 1e-2\n",
    "    portion = 0.25\n",
    "    pretrain(txt_file)\n",
    "    \n",
    "    n_epoch = 100\n",
    "    lr = 1e-3\n",
    "    fine_tune(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import make_grid, save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "\n",
    "\n",
    "img_size = 64\n",
    "batch_size = 100\n",
    "num_threads = 8\n",
    "lat_dim = 10\n",
    "\n",
    "beta = 4\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contructing dataset from the generated sample data from the 'SamplesGenerator.ipynb'\n",
    "\n",
    "\n",
    "class SampledData(Dataset):\n",
    "    def __init__(self, mode, file='data/dsprites/base.pt'):\n",
    "        samples = torch.load(file)\n",
    "        img = samples[:, :-1]\n",
    "        label = samples[:, -1]\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.x = img[:-batch_size*100]\n",
    "            self.y = label[:-batch_size*100].type(dtype=torch.int64)\n",
    "        elif self.mode == 'test':\n",
    "            self.x = img[-batch_size*100:]\n",
    "            self.y = label[-batch_size*100:].type(dtype=torch.int64)\n",
    "                    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader for generated samples\n",
    "\n",
    "def samples_loader():\n",
    "    train_data = SampledData('train')\n",
    "    test_data = SampledData('test')\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, num_workers=num_threads)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_threads)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function mostly for visualization purpose\n",
    "\n",
    "\n",
    "def param_fix(layer):\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def plot_loss_curve(generated, train_list, test_list):\n",
    "    fig = plt.figure()       \n",
    "    plt.plot(train_list, 'r-', label='train loss')\n",
    "    plt.plot(test_list, 'r--', label='test loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/'+generated+'/loss_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "def plot_accur_curve(accur_list):\n",
    "    fig = plt.figure()       \n",
    "    plt.plot(accur_list, 'b-', label='accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/accur/accuracys.png')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contructing BetaVAE model which have been appeared in 'BetaVAE(shapes).ipynb'\n",
    "\n",
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, supervised, generated='Bernoulli', mode='learn'):\n",
    "        super(BetaVAE, self).__init__()\n",
    "        self.supervised = supervised\n",
    "        self.generated = generated\n",
    "        self.mode = mode\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.encoder = nn.Sequential(nn.Linear(4096, 1200),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(1200, 1200),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(1200, lat_dim*2))\n",
    "                                    \n",
    "        self.decoder = nn.Sequential(nn.Linear(lat_dim, 1200),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(1200, 1200),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(1200, 1200),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(1200, 4096))\n",
    "        \n",
    "        self.layer = nn.Sequential(nn.Linear(lat_dim, 5),\n",
    "                                   nn.Softmax())\n",
    "        \n",
    "    def reparametrize(self, z_mu, z_log_var):\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn(std.size()).to(device)\n",
    "        return z_mu + std * eps\n",
    "\n",
    "    def encoderNet(self, x):\n",
    "        code = self.encoder(x)\n",
    "        z_mu = code[:, :lat_dim]\n",
    "        z_log_var = code[:, lat_dim:]\n",
    "        z = self.reparametrize(z_mu, z_log_var)\n",
    "        self.kl = -0.5 * ((1 + z_log_var) - z_mu * z_mu - torch.exp(z_log_var)).mean(dim=0).sum()\n",
    "        return z\n",
    "\n",
    "    def decoderNet(self, z):\n",
    "        h = self.decoder(z)\n",
    "        x_ = self.sigmoid(h)\n",
    "        return x_\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.mode == 'learn':\n",
    "            self.z = self.encoderNet(x)\n",
    "            if not self.supervised:\n",
    "                self.x_ = self.decoderNet(self.z)\n",
    "                self.recon = -(x * torch.log(self.x_ + 1e-10) + (1 - x) * torch.log(1 - self.x_ + 1e-10)).mean(dim=0).sum()\n",
    "                return self.x_, self.recon, self.kl\n",
    "            else:\n",
    "                self.factors = self.layer(self.z)\n",
    "                return self.factors\n",
    "        elif self.mode == 'generate':\n",
    "            self.x_ = self.decoderNet(x)\n",
    "            return self.x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for training linear classifier with samples data\n",
    "\n",
    "def Shapes_evaluate(dist='Bernoulli'):\n",
    "    train_loader, test_loader = samples_loader()\n",
    "    model = BetaVAE(supervised=True, generated=dist).to(device)\n",
    "    #model.load_state_dict(torch.load('models/'+dist+'/BetaVAE.pt')) \n",
    "    model.load_state_dict(torch.load('models/'+dist+'/Classifiers.pt')) \n",
    "    param_fix(model.encoder)\n",
    "    param_fix(model.decoder)\n",
    "    #print(model.supervised)\n",
    "\n",
    "    model.mode = 'learn'\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name)\n",
    "    \n",
    "    train_list, test_list, accur_list = [], [], []\n",
    "    \n",
    "    with open('accur/accur_list.txt', 'rb') as f:\n",
    "        accur_list = pickle.load(f)\n",
    "    \n",
    "    for epoch in range(n_epoch):        \n",
    "        train_loss, test_loss = 0.0, 0.0\n",
    "        cnt = 0\n",
    "        for x, y in train_loader:\n",
    "            cnt += 1\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            half = int(0.5*batch_size)\n",
    "            batch1 = inputs[:half]\n",
    "            batch2 = inputs[-half:]\n",
    "            \n",
    "            z1 = model.encoderNet(batch1)\n",
    "            z2 = model.encoderNet(batch2)\n",
    "            z_diff = torch.abs(z1-z2)\n",
    "            z_mean = z_diff.mean(dim=0).view(1, -1)\n",
    "            label = labels[0].view(-1)\n",
    "            \n",
    "            factor = model.layer(z_mean)\n",
    "            loss = criterion(factor, label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss\n",
    "        \n",
    "        train_loss = train_loss / cnt\n",
    "        train_list.append(train_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            cnt = 0\n",
    "            accur = 0\n",
    "            for x, y in test_loader:\n",
    "                cnt += 1\n",
    "                inputs, labels = x.to(device), y.to(device)\n",
    "                half = int(0.5*batch_size)\n",
    "                batch1 = inputs[:half]\n",
    "                batch2 = inputs[-half:]\n",
    "            \n",
    "                z1 = model.encoderNet(batch1)\n",
    "                z2 = model.encoderNet(batch2)\n",
    "                z_diff = torch.abs(z1-z2)\n",
    "                z_mean = z_diff.mean(dim=0).view(1, -1)\n",
    "                label = labels[0].view(-1)\n",
    "            \n",
    "                factor = model.layer(z_mean)\n",
    "                loss = criterion(factor, label)\n",
    "            \n",
    "                test_loss += loss\n",
    "                accur += sum(torch.argmax(factor, dim=1) == label)     \n",
    "    \n",
    "            test_loss = test_loss / cnt\n",
    "            test_list.append(test_loss)\n",
    "                    \n",
    "            accuracy = accur.item() / cnt * 100\n",
    "            accur_list.append(accuracy)\n",
    "            \n",
    "        torch.save(model.state_dict(), 'models/'+dist+'/Classifiers.pt')\n",
    "        \n",
    "        with open('accur/accur_list.txt', 'wb') as f:\n",
    "            pickle.dump(accur_list, f)\n",
    "    \n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print('[Epoch %d] train_loss: %.3f, test_loss: %.3f' \n",
    "                  % (epoch+1, train_loss, test_loss))\n",
    "\n",
    "    print('accuracy is ', max(accur_list),'%')\n",
    "    plot_loss_curve(model.generated, train_list, test_list)\n",
    "    plot_accur_curve(accur_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd experiment : running code for training linear classifier so that calculate the disentanglement metric via accuracy\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    print(device)\n",
    "    lr = 1e-2\n",
    "    n_epoch = 100\n",
    "    Shapes_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}